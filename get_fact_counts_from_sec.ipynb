{
 "metadata": {
  "name": "",
  "signature": "sha256:adf6276cfa232a7c1387ba9f2b4b92f49db904aaf10f1e925547c76f9d21882c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2, re, csv, sys\n",
      "import lxml.etree as etree\n",
      "from io import StringIO\n",
      "from bs4 import BeautifulSoup\n",
      "from datetime import datetime\n",
      "from collections import defaultdict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# destination file\n",
      "pstart = datetime.now()\n",
      "dest = open('C:/cygwin64/home/Mike.Breecher/sec-scraper/test/fact_counts.csv', 'w')\n",
      "writer = csv.writer(dest)\n",
      "#error file\n",
      "error = open('C:/cygwin64/home/Mike.Breecher/sec-scraper/test/errors.csv', 'w')\n",
      "error_write = csv.writer(error)\n",
      "\n",
      "with open('C:/cygwin64/home/Mike.Breecher/sec-scraper/test/ciks.csv', 'rb') as f:\n",
      "    reader = csv.reader(f)\n",
      "    for i, line in enumerate(reader):\n",
      "        # for each cik, read html page on sec\n",
      "        testcik = str(line[0])\n",
      "        print \"cik\" + \" \" + testcik\n",
      "        url = 'http://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=' + testcik + '&type=10&dateb=&owner=exclude&count=40'\n",
      "        u = urllib2.urlopen(url)\n",
      "        try:\n",
      "            html = u.read()\n",
      "            \n",
      "            if 'No matching CIK' in html:\n",
      "                print line[0] + ' is an invalid CIK'\n",
      "                error_result = [line[0], 'invalid CIK']\n",
      "                error_write.writerow(error_result)\n",
      "            else:\n",
      "                split_name = html.split(\"<span class=\\\"companyName\\\">\")\n",
      "                if len(split_name) > 1:\n",
      "                    split_name = split_name[1].split(\"<\")\n",
      "                    split_name = split_name[0].strip\n",
      "                else:\n",
      "                    split_name = \"not found\"\n",
      "                    \n",
      "                split_links = html.split(\"\\\" id=\\\"documentsbutton\\\"\");\n",
      "                s = 0\n",
      "                result = 0\n",
      "                while True:\n",
      "                    # check that we still have documents links and haven't gotten our 4 filings\n",
      "                    if s >= len(split_links) or result >= 4:\n",
      "                        break\n",
      "                    else:\n",
      "                        split_xbrl = split_links[s].split(\"<a href=\\\"\")\n",
      "                        s = s + 1\n",
      "                        \n",
      "                        #then check to see if we have an href to follow\n",
      "                        if 'Archives' in split_xbrl[len(split_xbrl) - 1]:\n",
      "                            # then follow the link to filings\n",
      "                            url = 'http://www.sec.gov' + split_xbrl[len(split_xbrl) - 1]\n",
      "                            u = urllib2.urlopen(url)\n",
      "                            \n",
      "                            try:\n",
      "                                html = u.read()\n",
      "                                u.close\n",
      "                                instance_split = html.split(\"EX-101.INS\")\n",
      "                                if len(instance_split) >= 2:\n",
      "                                    split_page = instance_split[len(instance_split)-2].split(\".xml</a>\",1);\n",
      "                                    split_xbrl = split_page[len(split_page)-2].split(\"<a href=\\\"\")\n",
      "                                    split_xml = split_xbrl[len(split_xbrl)-1].split(\"\\\">\")\n",
      "                                    \n",
      "                                    #grab the filing date\n",
      "                                    split_date = html.split(\"<div class=\\\"infoHead\\\">Filing Date</div>\")\n",
      "                                    split_date = split_date[1].split(\"<div class=\\\"infoHead\\\">Accepted</div>\")\n",
      "                                    split_date = split_date[0].replace(\"<div class=\\\"info\\\">\",\"\")\n",
      "                                    split_date = split_date.replace(\"</div>\",\"\")\n",
      "                                    split_date = split_date.strip()\n",
      "\n",
      "                                    doctype = \"\"\n",
      "                                    if \"10-K\" in html:\n",
      "                                        doctype = \"10-K\"\n",
      "                                    elif \"10-Q\" in html:\n",
      "                                        doctype = \"10-Q\"\n",
      "\n",
      "                                    # then follow the link to the instance file\n",
      "                                    url = 'http://www.sec.gov' + split_xml[0]\n",
      "                                    u = urllib2.urlopen(url)\n",
      "\n",
      "                                    if not \"10-Q/A\" in html and not \"10-K/A\" in html: \n",
      "                                        try:\n",
      "                                            html = u.read()\n",
      "                                            html = html.split(\"-->\")\n",
      "                                            xml = \"\"\n",
      "                                            for element in html:\n",
      "                                                sub = element.split(\"<!--\")\n",
      "                                                xml = xml + sub[0] \n",
      "\n",
      "                                            try:\n",
      "                                                p = etree.XMLParser(huge_tree=True)\n",
      "                                                xmldoc = etree.fromstring(xml, p)\n",
      "                                                if len(xmldoc) > 10:\n",
      "                                                    #for node in list(xmldoc.iter()):\n",
      "                                                    std_list = []\n",
      "                                                    ext_list = []\n",
      "                                                    ignored = []\n",
      "                                                    for node in list(xmldoc):\n",
      "                                                        if node.prefix is None or len(node.prefix) == 0:  \n",
      "                                                            ignored.append(node.tag)\n",
      "                                                        elif 'dei' in node.prefix or 'us-gaap' in node.prefix:\n",
      "                                                            std_list.append(node.tag)\n",
      "                                                        elif not 'xbrli' in node.prefix and not 'link' in node.prefix:\n",
      "                                                            ext_list.append(node.tag)\n",
      "                                                        else:\n",
      "                                                            ignored.append(node.tag)\n",
      "                                                        #print (node.prefix + \" \" + node.tag)\n",
      "                                                        #print node.tag\n",
      "                                                        #print node.text\n",
      "                                                        #mylist.append(node.prefix)\n",
      "                                                    if len(std_list) > 0:\n",
      "                                                        fact_count = len(std_list) + len(ext_list)\n",
      "                                                        print \"\\\"\" + testcik + \"\\\",\\\"\" + doctype+ \"\\\",\\\"\" + split_date + \"\\\",\\\"\" + str(fact_count) + \"\\\",\\\"\" + str(len(set(std_list))) + \"\\\",\\\"\" + str(len(set(ext_list))) + \"\\\"\" \n",
      "                                                        output = [testcik, doctype, split_date, fact_count, len(set(std_list)), len(set(ext_list))]\n",
      "                                                        writer.writerow(output)\n",
      "                                                        set(ignored)\n",
      "                                                        result = result + 1\n",
      "                                            finally:\n",
      "                                                pass\n",
      "                                        finally:\n",
      "                                            u.close\n",
      "                            finally:\n",
      "                                u.close\n",
      "        finally:\n",
      "            u.close\n",
      "dest.close()\n",
      "#prefixes = dict(list(xmldoc.iter()).prefix)\n",
      "#print prefixes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#troubleshooting block\n",
      "x = 0\n",
      "# for each cik, read html page on sec\n",
      "testcik = str(83246)\n",
      "print \"cik\" + \" \" + testcik\n",
      "url = 'http://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=' + testcik + '&type=10&dateb=&owner=exclude&count=40'\n",
      "u = urllib2.urlopen(url)\n",
      "try:\n",
      "    html = u.read()\n",
      "    \n",
      "    if 'No matching CIK' in html:\n",
      "        print testcik + ' is an invalid CIK'\n",
      "        #error_result = [line[0], 'invalid CIK']\n",
      "        #error_write.writerow(error_result)\n",
      "    else:\n",
      "        split_name = html.split(\"<span class=\\\"companyName\\\">\")\n",
      "        if len(split_name) > 1:\n",
      "            split_name = split_name[1].split(\"<\")\n",
      "            split_name = split_name[0].strip()\n",
      "        else:\n",
      "            split_name = \"not found\"\n",
      "        print split_name\n",
      "        \n",
      "        split_links = html.split(\"\\\" id=\\\"documentsbutton\\\"\");\n",
      "        split_xbrl = split_links[0].split(\"<a href=\\\"\")\n",
      "        s = 0\n",
      "        result = 0\n",
      "        while True:\n",
      "            # check that we still have documents links and haven't gotten our 4 filings\n",
      "            if s >= len(split_links) or result >= 4:\n",
      "                break\n",
      "            else:\n",
      "                split_xbrl = split_links[s].split(\"<a href=\\\"\")\n",
      "                s = s + 1\n",
      "\n",
      "                #then check to see if we have an href to follow\n",
      "                if 'Archives' in split_xbrl[len(split_xbrl) - 1]:\n",
      "                    # then follow the link to filings\n",
      "                    url = 'http://www.sec.gov' + split_xbrl[len(split_xbrl) - 1]\n",
      "                    u = urllib2.urlopen(url)\n",
      "\n",
      "                    try:\n",
      "                        html = u.read()\n",
      "                        u.close\n",
      "                        instance_split = html.split(\"EX-101.INS\")\n",
      "                        if len(instance_split) >= 2:\n",
      "                            split_page = instance_split[len(instance_split)-2].split(\".xml</a>\",1);\n",
      "                            split_xbrl = split_page[len(split_page)-2].split(\"<a href=\\\"\")\n",
      "                            split_xml = split_xbrl[len(split_xbrl)-1].split(\"\\\">\")\n",
      "\n",
      "                            #grab the filing date\n",
      "                            split_date = html.split(\"<div class=\\\"infoHead\\\">Filing Date</div>\")\n",
      "                            split_date = split_date[1].split(\"<div class=\\\"infoHead\\\">Accepted</div>\")\n",
      "                            split_date = split_date[0].replace(\"<div class=\\\"info\\\">\",\"\")\n",
      "                            split_date = split_date.replace(\"</div>\",\"\")\n",
      "                            split_date = split_date.strip()\n",
      "\n",
      "                            # then follow the link to the instance file\n",
      "                            url = 'http://www.sec.gov' + split_xml[0]\n",
      "                            u = urllib2.urlopen(url)\n",
      "\n",
      "                            if not \"10-Q/A\" in html and not \"10-K/A\" in html: \n",
      "                                try:\n",
      "                                    html = u.read()\n",
      "                                    html = html.split(\"-->\")\n",
      "                                    xml = \"\"\n",
      "                                    for element in html:\n",
      "                                        sub = element.split(\"<!--\")\n",
      "                                        xml = xml + sub[0] \n",
      "\n",
      "                                    try:\n",
      "                                        p = etree.XMLParser(huge_tree=True)\n",
      "                                        xmldoc = etree.fromstring(xml, p)\n",
      "                                        if len(xmldoc) > 10:\n",
      "\n",
      "                                            #for node in list(xmldoc.iter()):\n",
      "                                            std_list = []\n",
      "                                            ext_list = []\n",
      "                                            ignored = []\n",
      "                                            for node in list(xmldoc):\n",
      "                                                if node.prefix is None or len(node.prefix) == 0:  \n",
      "                                                    ignored.append(node.tag)\n",
      "                                                elif 'dei' in node.prefix or 'us-gaap' in node.prefix:\n",
      "                                                    std_list.append(node.tag)\n",
      "                                                elif not 'xbrl' in node.prefix and not 'link' in node.prefix:\n",
      "                                                    ext_list.append(node.tag)\n",
      "                                                else:\n",
      "                                                    ignored.append(node.tag)\n",
      "                                                #print (node.prefix + \" \" + node.tag)\n",
      "                                                #print node.tag\n",
      "                                                #print node.text\n",
      "                                                #mylist.append(node.prefix)\n",
      "                                            if len(std_list) > 0:\n",
      "                                                fact_count = len(std_list) + len(ext_list)\n",
      "                                                print str(fact_count) + ' ' + \"facts\"\n",
      "                                                print \"filed on:\" + split_date\n",
      "                                                print str(len(set(std_list))) + ' ' + \"standard concepts\"\n",
      "                                                print str(len(set(ext_list))) + ' ' + \"extended concepts\"\n",
      "                                                set(ignored)\n",
      "                                                result = result + 1\n",
      "                                    finally:\n",
      "                                        pass\n",
      "                                finally:\n",
      "                                    u.close\n",
      "                    finally:\n",
      "                        u.close\n",
      "finally:\n",
      "    pass\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "cik 83246\n",
        "HSBC USA INC /MD/"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5778 facts"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "filed on:2015-02-23\n",
        "615 standard concepts\n",
        "313 extended concepts\n",
        "4630 facts"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "filed on:2014-11-03\n",
        "369 standard concepts\n",
        "174 extended concepts\n",
        "4650 facts"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "filed on:2014-08-04\n",
        "329 standard concepts\n",
        "220 extended concepts\n",
        "3766 facts"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "filed on:2014-05-07\n",
        "324 standard concepts\n",
        "227 extended concepts\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "while True:\n",
      "    # check that we still have documents links and haven't gotten our 4 filings\n",
      "    if s >= len(split_page) or result >= 4:\n",
      "        break\n",
      "\n",
      "    split_xbrl = split_page[s].split(\"<a href=\\\"\")\n",
      "    s = s + 1\n",
      "    print split_xbrl"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 203
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "split_page = instance_split.split(\".xml</a>\",1);\n",
      "split_xbrl = split_page.split(\"<a href=\\\"\")\n",
      "split_xml = split_xbrl.split(\"\\\">\")\n",
      "for node in split_xml:\n",
      "    print node"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'list' object has no attribute 'split'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-42-70b5bf1b2497>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msplit_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance_split\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".xml</a>\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msplit_xbrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_page\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"<a href=\\\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msplit_xml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_xbrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\\">\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msplit_xml\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
       ]
      }
     ],
     "prompt_number": 42
    }
   ],
   "metadata": {}
  }
 ]
}